{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgf7mqEmqDz1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# SEMTM0016 Week 20: Value based Reinforcement Learning - MDP and Model-based Methods\n",
        "\n",
        "## Instructions\n",
        "1.   To start this notebook, please duplicate this notebook at first:\n",
        "  - Choose \"File => Save a copy in Drive\" and open/run it in Colab.\n",
        "  - Or you can download the notebook and run it in your local jupyter notebook server.\n",
        "2.   For the coding assignment and practice, please write your code at `### TODO ###` blocks or in a new cell. For analysis report, you are free to use as many blocks as you need.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section I: The basics of Markov decision process (MDP)\n",
        "\n",
        "A Markov decision process (MDP) is a Markov reward process with\n",
        "decisions. It is an environment in which all states are Markov dependent, which means that the future is independent of the past given the present. A state $S_{t}$ is Markov if and only if\n",
        "\n",
        "$$\n",
        "\\mathbb{P}\\left[S_{t+1} \\mid S_{t}\\right]=\\mathbb{P}\\left[S_{t+1} \\mid S_{1}, \\ldots, S_{t}\\right]\n",
        "$$\n",
        "\n",
        "A Markov Decision Process is a tuple $\\langle\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma\\rangle$\n",
        "* S is a finite set of states\n",
        "* A is a finite set of actions\n",
        "* P is a state transition probability matrix, $\\mathcal{P}_{s s^{\\prime}}^{a}=\\mathbb{P}\\left[S_{t+1}=s^{\\prime} \\mid S_{t}=s, A_{t}=a\\right]$\n",
        "* R is a reward function, $\\mathcal{R}_{s}^{a}=\\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s, A_{t}=a\\right]$\n",
        "* $\\gamma$ is a discount factor $\\gamma \\in[0,1]$.\n",
        "\n",
        "The state-value function $v_{\\pi}(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\pi$\n",
        "\n",
        "$$\n",
        "V_{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s\\right]\n",
        "$$\n",
        "\n",
        "The action-value function $q_{\\pi}(s, a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$\n",
        "\n",
        "\n",
        "$$\n",
        "Q_{\\pi}(s, a)=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s, A_{t}=a\\right]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "where a policy $\\pi$ is a distribution over actions given states,\n",
        "\n",
        "$$\n",
        "\\pi(a \\mid s)=\\mathbb{P}\\left[A_{t}=a \\mid S_{t}=s\\right]\n",
        "$$\n",
        "\n",
        "From the definition above, we can derive the Bellman Expectation Equation over value function:\n",
        "\n",
        "$$\n",
        "V_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) Q_{\\pi}(s, a) \\tag{1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Q_{\\pi}(s, a)=\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} V_{\\pi}\\left(s^{\\prime}\\right) \\tag{2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left(\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} V{\\pi}\\left(s^{\\prime}\\right)\\right) \\tag{3}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "Q_{\\pi}(s, a)=\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} \\sum_{a^{\\prime} \\in \\mathcal{A}} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) Q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\tag{4}\n",
        "$$\n",
        "\n",
        "\n",
        "The optimal state-value function $V_{*}(s)$ is the maximum value function over all policies\n",
        "\n",
        "$$\n",
        "V_{*}(s)=\\max _{\\pi} V_{\\pi}(s)\n",
        "$$\n",
        "\n",
        "The optimal action-value function $Q_{*}(s, a)$ is the maximum action-value function over all policies\n",
        "\n",
        "$$\n",
        "Q_{*}(s, a)=\\max _{\\pi} Q_{\\pi}(s, a)\n",
        "$$\n",
        "\n",
        "And the Bellman Optimal Equation can be derived as:\n",
        "$$\n",
        "V_{*}(s)=\\max _{a} Q_{*}(s, a) \\tag{5}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Q_{*}(s, a)=\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} V_{*}\\left(s^{\\prime}\\right) \\tag{6}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V_{*}(s)=\\max _{a} \\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} V_{*}\\left(s^{\\prime}\\right) \\tag{7}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "Q_{*}(s, a)=\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} \\max _{a^{\\prime}} Q_{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\tag{8}\n",
        "$$"
      ],
      "metadata": {
        "id": "XhE32iuVleXj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNmdUiqRqw8Y"
      },
      "source": [
        "# Section II: Model-based learning (Policy Iteration/Value Iteration)\n",
        "\n",
        "When the environment is known for us, we can solve for the optimal value function and optimal policy by dynamic programming. Two classical algorithms of dynamic programming is Policy Iteration (PI) and Value Iteration (VI).\n",
        "\n",
        "The core of **policy iteration** is the alternating procedure between policy evaluation and policy improvement.\n",
        "* In policy evaluation phase, we need to estimate the value function of current policy $\\pi$ by iterative applying the Bellman Expectation Equation (equation (3)) until the value function $V_{\\pi}$ converges.\n",
        "* In policy improvement phase, we improve the policy from $\\pi$ to $\\pi^{\\prime}$ by acting greedily with respect to $V_{\\pi}$.\n",
        "\n",
        "The detailed algorithm procedures are shown as follows:\n",
        "\n",
        "<div align=center><img src=\"https://i.stack.imgur.com/kKZx7.png\" width=\"500px\" /></div>\n",
        "\n",
        "The core of **value iteration** is to directly obtain the optimal value function.\n",
        "* Value iteration aims at obtaining optimal value function to derive the final optimal policy. It iteratively applies Bellman Optimal Equation update (equation (7)) until the optimal value function converges.\n",
        "\n",
        "The detailed algorithm procedures are shown as follows:\n",
        "\n",
        "<div align=center><img src=\"https://i.stack.imgur.com/CAAu5.png\" width=\"500px\" /></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Policy Iteration on the Shortest Path Problem in Gridworld\n",
        "\n",
        "In this part, we use the shortest path problem in gridworld to illustrate how policy iteration works.\n",
        "\n",
        "The shortest path problem utilises an $N\\times N$ gridworld as its map, where $N$ refers to the length. The following figure shows an example when $N=4$.\n",
        "\n",
        "<div align=center><img src=\"https://miro.medium.com/max/1290/1*aP4jWZ0vbCzH4Gb2MqedtQ.png\" width=\"500px\" /></div>\n",
        "\n",
        "This environment has the following chacteristics:\n",
        "* An undiscounted episodic MDP (with $\\gamma=1$)\n",
        "* 14 non-terminal states from 1 to 14\n",
        "* Two terminal states (shaded squares)\n",
        "* Actions leading out of the grid leave state unchanged\n",
        "* Reward is -1 until the terminal state is reached\n",
        "\n",
        "In this part, **you need to do the following things**:\n",
        "* Complete the implementation of policy iteration on this problem with $N=6, \\gamma=1.0$. To finish the implementation, you need to complete 4 functions. **next_state**: given current row/column index and action, return the row/column index of next state. **get_q_matrix**:  calculate Q value matrix from V value matrix. **evaluate_policy**:  evaluate the V value matrix given a fixed policy by iteratively applying Bellman Expectation Equation update. **get_optimal**: Generate the greedy policy given the Q value matrix.\n",
        "* Visualising the final policy.\n"
      ],
      "metadata": {
        "id": "2WgqacIf2qTH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDDTZv24ebxd"
      },
      "source": [
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AV7eimGevHx"
      },
      "source": [
        "### policy iteration\n",
        "\n",
        "### For action, 0 for up, 1 for left, 2 for down and 3 for right\n",
        "### the policy is dim*dim*4\n",
        "### The V matrix is dim*dim\n",
        "### The Q matrix is dim*dim*4\n",
        "\n",
        "dim = 6\n",
        "gamma = 1.0\n",
        "eps = 1e-4\n",
        "\n",
        "def next_state(i, j, action, num):\n",
        "  ### i refers to the row index\n",
        "  ### j refers to column index\n",
        "  ### action is in [0,1,2,3],\n",
        "  ### num is the dimension of square.\n",
        "\n",
        "  ### TODO: return the index of next state, remember to handle corner case\n",
        "  ### Everytime when the agent locates at the edge of the square,\n",
        "  ### illegal action will not change the position of agent\n",
        "  ### For example: (i=0, j=2), (row 0 column 2) when taking action = 0(up), the position will still be (i=0, j=2)\n",
        "  ### (i=1, j=2), (row 1 column 2) when taking action = 0(up), the position will still be (i=0, j=2)\n",
        "\n",
        "\n",
        "\n",
        "  ### END\n",
        "  return next_i, next_j\n",
        "\n",
        "def get_q_matrix(value_matrix):# calculate Q value from V\n",
        "  num = len(value_matrix)\n",
        "  q_matrix = np.zeros([num, num, 4])\n",
        "  for i in range(num):\n",
        "    for j in range(num):\n",
        "      if (i == 0 and j == 0) or (i == num-1 and j == num-1): # skip terminal state\n",
        "        continue\n",
        "      for action in range(4):\n",
        "        next_i, next_j = next_state(i, j, action, num)\n",
        "        if (next_i == 0 and next_j == 0) or (next_i == num-1 and next_j == num-1):\n",
        "          next_value = 0\n",
        "        else:\n",
        "          next_value = value_matrix[next_i, next_j]\n",
        "        ### TODO : calculate the Q function q_matrix[i,j,action], refer to equation (2)\n",
        "        ### END\n",
        "  return q_matrix\n",
        "\n",
        "def evaluate_policy(policy, eps):# policy evaluation by iteratively applying Bellman Equation update\n",
        "  num = len(policy)\n",
        "  value_matrix = np.random.randn(num,num)\n",
        "  value_matrix[0,0] = 0 # for terminal state, set state value as 0\n",
        "  value_matrix[-1,-1] = 0 # for terminal state, set state value as 0\n",
        "  while True:\n",
        "    value_prev = copy.deepcopy(value_matrix)\n",
        "    for i in range(num):\n",
        "      for j in range(num):\n",
        "        value = 0\n",
        "        if (i == 0 and j == 0) or (i == num-1 and j == num-1): # skip terminal state\n",
        "          continue\n",
        "        for action in range(4): # 0 for up, 1 for left, 2 for down and 3 for right\n",
        "          next_i, next_j = next_state(i, j, action, num)\n",
        "          if (next_i == 0 and next_j == 0) or (next_i == num-1 and next_j == num-1):\n",
        "            next_value = 0\n",
        "          else:\n",
        "            next_value = value_prev[next_i, next_j]\n",
        "          ### TODO : calculate the state value function for value_matrix[i][j], refer to equation (3)\n",
        "\n",
        "          ### END\n",
        "        value_matrix[i][j] = value\n",
        "    diff = np.mean((value_prev-value_matrix)**2) ## calculate the difference\n",
        "    if diff < eps: ## if less then the thresold, break\n",
        "      break\n",
        "  return value_matrix\n",
        "\n",
        "def get_optimal(q_matrix): #greedy policy from Q/policy improvement\n",
        "  num = len(q_matrix)\n",
        "  policy = np.ones([num,num,4])/4\n",
        "  for i in range(num):\n",
        "    for j in range(num):\n",
        "      # ignore terminal state\n",
        "      if (i == 0 and j == 0) or (i == num-1 and j == num-1): # skip terminal state\n",
        "          continue\n",
        "      q_value = q_matrix[i,j]\n",
        "\n",
        "      ### TODO : Get greedy policy given Q matrix\n",
        "      ### Let the action with the same max Q value has the same probability, while the rest are 0\n",
        "      ### For example: with Q value = [1,1,0], the policy is [0.5, 0.5, 0]\n",
        "      ### With Q value = [0,1,0], thc policy is [0, 1, 0]\n",
        "\n",
        "      ### END\n",
        "      policy[i, j] = policy_state\n",
        "  return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(dim, eps):\n",
        "  policy = np.ones([dim,dim,4])/4 # uniform policy\n",
        "  while True:\n",
        "    prev_policy = copy.deepcopy(policy)\n",
        "    value_matrix = evaluate_policy(policy, eps)# policy evaluation\n",
        "    q_matrix = get_q_matrix(value_matrix)# calculate Q value from V\n",
        "    policy = get_optimal(q_matrix)# greedy policy from Q\n",
        "    diff = np.mean((prev_policy-policy)**2)# whether the difference is smaller than thresold\n",
        "    if diff < eps:\n",
        "      print('converge')\n",
        "      break\n",
        "  return policy, value_matrix"
      ],
      "metadata": {
        "id": "MxH5jWGEyY0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy, value_matrix = policy_iteration(dim, eps)\n",
        "print(policy)\n",
        "print(value_matrix)"
      ],
      "metadata": {
        "id": "iZIg6rGOG9OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualising the learned policy"
      ],
      "metadata": {
        "id": "hPHBkwZI-FJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Visualising the learned policy\n",
        "from matplotlib.pyplot import MultipleLocator\n",
        "import matplotlib.pyplot as plt\n",
        "def visualse(policy):\n",
        "  plt.figure(figsize=(dim,dim))\n",
        "  ax = plt.gca()\n",
        "  x_major_locator=MultipleLocator(1)\n",
        "  ax.xaxis.set_major_locator(x_major_locator)\n",
        "  ax.yaxis.set_major_locator(x_major_locator)\n",
        "  plt.xlim(0, dim)\n",
        "  plt.ylim(0, dim)\n",
        "\n",
        "  for i in range(dim):\n",
        "    for j in range(dim):\n",
        "      if (i == 0 and j == 0) or (i == dim-1 and j == dim-1): # skip terminal state\n",
        "        continue\n",
        "      for action, action_prob in enumerate(policy[i,j]):\n",
        "        if action_prob > 0:\n",
        "          if action == 0:\n",
        "            x_distance = 0\n",
        "            y_distance = 0.3\n",
        "          elif action == 1:\n",
        "            x_distance = -0.3\n",
        "            y_distance = 0\n",
        "          elif action == 2:\n",
        "            x_distance = 0\n",
        "            y_distance = -0.3\n",
        "          elif action == 3:\n",
        "            x_distance = 0.3\n",
        "            y_distance = 0\n",
        "          plt.arrow(j+0.5, dim-i-0.5, x_distance, y_distance, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
        "        else:\n",
        "          continue\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "visualse(policy)"
      ],
      "metadata": {
        "id": "kxmL2X9tCgr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Value Iteration on the Shortest Path Problem in Gridworld\n",
        "\n",
        "In this part, we use the shortest path problem in gridworld to illustrate how value iteration works.\n",
        "\n",
        "In this part, **you need to do the following things**:\n",
        "* Complete the implementation of value iteration on this problem with $N=5, \\gamma=0.9$. To finish the implementation, you need to complete 1 function. **update_optimal_value_function**: conduct optimal Bellman Equation update iteratively.\n",
        "* Visualising the final policy.\n"
      ],
      "metadata": {
        "id": "-pSGGwNZ-YIj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmhZ29Gze1VP"
      },
      "source": [
        "gamma = 0.9\n",
        "dim = 5\n",
        "\n",
        "### value iteration\n",
        "def update_optimal_value_function(optimal_value_matrix):\n",
        "  value_matrix = copy.deepcopy(optimal_value_matrix)\n",
        "  num = len(optimal_value_matrix)\n",
        "  for i in range(num):\n",
        "    for j in range(num):\n",
        "      value = 0\n",
        "      if (i == 0 and j == 0) or (i == num-1 and j == num-1): # skip terminal state\n",
        "        continue\n",
        "      max_value = -np.inf\n",
        "      for action in range(4): # 0 for up, 1 for left, 2 for down and 3 for right\n",
        "        next_i, next_j = next_state(i, j, action, num)\n",
        "        if (next_i == 0 and next_j == 0) or (next_i == num-1 and next_j == num-1):\n",
        "          next_value = 0\n",
        "        else:\n",
        "          next_value = value_matrix[next_i, next_j]\n",
        "        ### TODO: Conduct Bellman optimal equation update, refer to equation 7\n",
        "\n",
        "        ### END\n",
        "      optimal_value_matrix[i][j] = max_value\n",
        "  return optimal_value_matrix\n",
        "\n",
        "def value_iteration():\n",
        "  optimal_value_matrix = np.random.randn(dim, dim)\n",
        "  optimal_value_matrix[0,0] = 0\n",
        "  optimal_value_matrix[-1,-1] = 0\n",
        "  while True:\n",
        "    prev_optimal_value_matrix = copy.deepcopy(optimal_value_matrix)\n",
        "    optimal_value_matrix = update_optimal_value_function(optimal_value_matrix) # Bellman optimal equation update\n",
        "    # get optimal q matrix from optimal V matrix, reuse previous function because equation (2) = (6)\n",
        "    optimal_q_matrix = get_q_matrix(optimal_value_matrix)\n",
        "    optimal_policy = get_optimal(optimal_q_matrix)# get optimal policy from optimal Q matrix\n",
        "    diff = np.mean((prev_optimal_value_matrix-optimal_value_matrix)**2) # calculate the difference\n",
        "    if diff < eps:\n",
        "      print('converge')\n",
        "      break\n",
        "  return optimal_policy, optimal_value_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy, optimal_value_matrix = value_iteration()\n",
        "print(optimal_policy)\n",
        "print(optimal_value_matrix)"
      ],
      "metadata": {
        "id": "h_2OQ9CM-12x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualse(optimal_policy)"
      ],
      "metadata": {
        "id": "aRBSXFwq-qKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reference\n",
        "The illustration of VI/PI refers to:\n",
        "* https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf\n",
        "\n",
        "The VI/PI algorithm refers to:\n",
        "* http://incompleteideas.net/book/RLbook2020.pdf\n",
        "\n",
        "Refer code to draw the grid plot in VI/PI:\n",
        "* https://www.kaggle.com/nvtnganfb/gridworld"
      ],
      "metadata": {
        "id": "S6L4XaIDlCZ4"
      }
    }
  ]
}